---
title: 複製網頁內容並儲存成資料庫
tags: python crawler
layout: article
aside:
  toc: true
sidebar:
  nav: layouts
date:  2022-10-12
modify_date: 2022-10-12 06:11:06
---

有些網頁會將數據直接寫在程式碼內容之中，因此需要將程式碼讀出來(page_source)，並將其轉成所要的格式，網頁之外觀如以下範例(新北市交通流量即時監測資訊)所示。

```json
{"success":true,"result":{"resource_id":"382000000A-000357-001","limit":2000,"total":702,"fields":[{"type":"text","id":"vdid"},{"type":"text","id":"datacollecttime"},{"type":"text","id":"status"},{"type":"text","id":"vsrid"},{"type":"text","id":"vsrdir"},{"type":"text","id":"speed"},{"type":"text","id":"laneoccupy"},{"type":"text","id":"carid"},{"type":"text","id":"volume"}],"records":[{"vdid":"65000V008130","datacollecttime":"2017/12/04 21:38:00","status":"3","vsrid":"0","vsrdir":"0","speed":"-99","laneoccupy":"-99","carid":"L","volume":"-99"},{"vdid":"65000V008130","datacollecttime":"2017/12/04 21:38:00","status":"3","vsrid":"0","vsrdir":"0", "speed":"-99", "laneoccupy":"-99","carid":"S","volume":"-99"},
...
```

## 程式碼中切割出所要字串
這些資訊(json或dict格式)可以在程式碼中找到：

```html
<html><head><link rel="alternate stylesheet" type="text/css" href="resource://gr
e-resources/plaintext.css" title="Wrap Long Lines"></head><body><pre>{…}</pre></body></html>
```

由於網頁程式碼是以<…>來撰寫，因此須以’>’及’<’來分割(split)，以截取出所要的字串。

```python
kuang@master /home/backup/data/ETC/NewtaipeiVD
$ cat getvd.py
from selenium import webdriver
import simplejson
from pandas import *

driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver") #Linux
url='http://data.ntpc.gov.tw/api/v1/rest/datastore/382000000A-000357-001'
driver.get(url)
fname='latest.json'
fl=open(fname,'w')
fl.write(driver.page_source)
driver.quit()
fl.close()
with open(fname) as ftext:
    s=[]
    for line in ftext:
        s.append(line.split('>'))
ss=[x for x in s[0][6].split('<')]
z=ss[0]
json_2_dict = simplejson.loads(z)
jj=json_2_dict['result']
df=DataFrame(jj['records'])
cols=["vdid","datacollecttime","status","vsrid","vsrdir","speed","laneoccupy","carid","volume"]
df[cols].set_index(['vdid']).to_csv('latest')
```
## 字串轉成字典

可以使用simplejson.loads將字串轉成字典，由於檢視該字典結構發現，是具有層次的，真正的數據是在第三層(result, records)之下，必須循序將其讀出來。由於字典可以轉成pandas資料庫，可以寫成csv輸出格式，方便後續整併處理。

## 定時批次執行

新北市VD數據是每5分鐘更新覆蓋在網頁上，因此必須每5分鐘讀取儲存。可以利用bash之date功能，讓系統判定是否5分鐘到了，若還沒就先暫停(sleep)，若到了，就執行前述python程式。

```bash
$ cat NewtpGetVD.cs
rm latest
A=`date +%M`;M=`date +%M|cut -c1`
if [ $M = 0 ] ;then A=`date +%M|cut -c2`; fi
oldM=$(( $A / 5 ))
newM=$(( $A / 5 ))
while true; do
while [[ $newM == $oldM ]]; do
A=`date +%M`;M=`date +%M|cut -c1`
if [ $M = 0 ] ;then A=`date +%M|cut -c2`; fi
newM=$(( $A / 5 ));sleep 60;done;
oldM=$newM
ymd=`date  --rfc-3339='date'`
python getvd.py
cat latest>>$ymd
rm latest
done
```

撰寫時須注意bash無法進行字串I2.2的計算(如 $(( 07 / 5))是無法執行的)，必須先判斷分鐘數是個位數還是10位數，10位數M=`date +%M|cut -c1`若是0，則是個位數，必須取其第2位來計算。

執行結果必須儲存在當日的檔案中，此處運用date來取年月日數，用cat … >> …來附加在檔案最後面。

### XML檔案格式數據之讀取
XML是一種常用網頁程式碼的類型，由於其數據儲存相對較為緊密，常常用來儲存即時更新之數據(以下範例為台北市交通局車輛流量監測數據)，因此其爬蟲程式不是重點，一般以wget即可完整下載(詳見app.doc中有關bash變數、字串去尾等相關說明。)，然而其解析過程卻貌似非常困難。

究其困難在於：並無統一格式可遵循、無組織(每一個監測點有不同資料項目數量)、也不是每一個欄位均有完整的欄位名稱及數值配對、(亦即有不少欄位是從來不會有內容的)、最後每一次網頁只有寫一次時間，必須另外加一時間之資料項目與內容到資料庫內。
### 循序讀取範例

理論上結構化之檔案可以批次直接讀取較為有效，不應循序讀取，然而如果分支太過嚴重只得如此。以下為網頁內容範例:

```html
<?xml version="1.0" encoding="UTF-8"?><VDInfoSet><ExchangeTime>2017/12/05T17:17:25 </ExchangeTime> <VDInfo><VDData><VDDevice><DeviceID>V0111C0</DeviceID><TimeInterval>5</TimeInterval><TotalOfLane>1</TotalOfLane><LaneData><LaneNO>0</LaneNO><Volume>143</Volume><AvgSpeed>41</AvgSpeed><AvgOccupancy>17.0</AvgOccupancy><Svolume>0.0</Svolume><Mvolume>143</Mvolume><Lvolume>0</Lvolume></LaneData></VDDevice></VDData></VDInfo><VDInfo><VDData><VDDevice><DeviceID>V0112A0</DeviceID><TimeInterval>5</TimeInterval><TotalOfLane>1</TotalOfLane><LaneData><LaneNO>0</LaneNO><Volume>94</Volume><AvgSpeed>22</AvgSpeed><AvgOccupancy>22.0</AvgOccupancy><Svolume>0.0</Svolume><Mvolume>93</Mvolume><Lvolume>1</Lvolume></LaneData></VDDevice></VDData></VDInfo><VDInfo><VDData><VDDevice><DeviceID>V0120C0</DeviceID><TimeInterval>5</TimeInterval><TotalOfLane>2</TotalOfLane><LaneData><LaneNO>0</LaneNO><Volume>69</Volume><AvgSpeed>64</AvgSpeed><AvgOccupancy>5.0</AvgOccupancy><Svolume>0.0</Svolume><Mvolume>65</Mvolume><Lvolume>4</Lvolume></LaneData><LaneData><LaneNO>1</LaneNO><Volume>111</Volume><AvgSpeed>62</AvgSpeed><AvgOccupancy>14.0</AvgOccupancy><Svolume>0.0</Svolume><Mvolume>107</Mvolume><Lvolume>4</LLvolume>
...
```

此處應用xml.etree.cElementTree模組來進行解析，詳細內容可以參考網友之說明，以下直接介紹範例程式。

```python
kuang@master /home/backup/data/ETC/TaipeiVD
$ cat getVD.py
from pandas import *
import xml.etree.cElementTree as ET
fname='GetVDDATA'
tree=ET.ElementTree(file=fname)
ttxt=[elem.text for elem in tree.iter()]
id=set([elem.text for elem in tree.iter(tag='DeviceID')])
cols=['DeviceID','TotalOfLane','LaneNO','Volume','AvgSpeed','AvgOccupancy','Svolume','Mvolume','Lvolume']
v=[]
for i in xrange(len(cols)):
    v.append([])
for i in id:
    ist=ttxt.index(i)
    ttl=int(ttxt[ist+2])
    for j in xrange(ttl):
        v[0].append(i)
        v[1].append(ttl)
        istt=ist+4+j*8
        for k in xrange(2,len(cols)):
            v[k].append(ttxt[istt+k-2])
d={}
for i in xrange(len(cols)):
    d.update({cols[i]:v[i]})

cols.append('ExchangeTime')
d.update({cols[-1]:[ttxt[1] for x in xrange(len(v[0]))]})
df=DataFrame(d)
df[cols].set_index('DeviceID').to_csv('latest')
```

cElementTree與ElementTree差別在實現過程是否使用C語言副程式，這會使程式的記憶體較小一些，效率好一些。檢查tree的內容可以使用tree.getroot()、elem.tag、elem.attrib、elem.text等指令來探索要解析的XML檔案，如範例程式中就是運用[elem.text for elem in tree.iter()]指令將所有網頁內容(數據內容部分)存成序列ttxt。

經過摸索之後，發現監測資料乃是以監測設施id為中心，有的設施有多條線道、有的只有單線道，而其有用的資訊除了時間須外加以外，其餘皆以循序方式，接在id的後面，必須以循序的方式小心讀取。

cols的數目雖然在每個監測點會略有不同，但順序總是相同，數量總是8的倍數(*總車道數TotalOfLane)，正好是7個變數與1個無用的LaneData共8個變數位置。
順序做好了，剩下就是序列名稱與對應關係了。此處先宣告9個空白、相同名字的2維序列，依序納入id、總車道數、以及其後的7個變數內容，這樣在編列成字典時就不需要再繁瑣撰寫，只要逐一將字典update即可。

如前所述，網頁只會在表頭處(第2個項目，即ttxt[1])寫一次時間	'ExchangeTime'，而不是每個id都會重複，並不符合資料庫型態，並且為了避免在讀取過程中造成干擾，因此在讀完之後再加一個字典項目即可。最後以csv形式輸出以利檔案整併。

### XML屬性資料的利用

使用ElementTree讀取XML資料時，元素除了標簽(鑰)與文字(值)之外，還有屬性欄位可以放置數據內容，作為下一層次資料的統一說明，如下(桃園市車輛偵測2018/11月底前的即時數據，高雄市格式亦相同)所示：

```html
kuang@master /home/backup/data/ETC/TaoyuanVD
$ more a
<XML_Head version="1.1" listname="VD ä¸€åˆ†é˜å‹•æ…‹è³‡è¨Š" updatetime="2017/12/06 09:55:10" interval="60
" xsi:noNamespaceSchemaLocation="http://211.72.255.245/VDClient/xsd_v11/VD_Value.xsd" xmlns:xs
i="http://www.w3.org/2001/XMLSchema-instance">
    <Infos>
        <Info vdid="68000V100110" status="0" datacollecttime="2017/12/06 09:55:10">
            <lane vsrdir="0" vsrid="0" speed="0" laneoccupy="0.0">
                <cars carid="L" volume="0"/>
                <cars carid="S" volume="0"/>
                <cars carid="M" volume="0"/>
            </lane>
            <lane vsrdir="0" vsrid="1" speed="0" laneoccupy="0.0">
                <cars carid="L" volume="0"/>
                <cars carid="S" volume="0"/>
                <cars carid="M" volume="0"/>
            </lane>
        </Info>
        <Info vdid="68000V100360" status="0" datacollecttime="2017/12/06 09:55:10">
            <lane vsrdir="0" vsrid="0" speed="0" laneoccupy="0.0">
                <cars carid="L" volume="0"/>
                <cars carid="S" volume="0"/>
                <cars carid="M" volume="0"/>
            </lane>
            <lane vsrdir="0" vsrid="1" speed="0" laneoccupy="0.0">
                <cars carid="L" volume="0"/>
                <cars carid="S" volume="0"/>
                <cars carid="M" volume="0"/>
            </lane>
            <lane vsrdir="1" vsrid="2" speed="0" laneoccupy="0.0">
                <cars carid="L" volume="0"/>
```                
XML_head標簽空一格後，跟著的就是所謂的屬性資料(elem.attrib)段落，在下一層Info之後，第3層lane之後，與第4層cars之後，都是該標籤的屬性。

ElemetTree將XML屬性的資訊內容(key=value)轉以python/json字典的型態({key:value})予以儲存，因此呼叫時必須以各個字典的鑰來代出其值。

可以運用elem.tag和elem.attrib將標籤與屬性的內容先儲存成序列，在本案例中elem.text沒有作用。如：

```python
    tree=ET.ElementTree(file=fname)
    ttag=[elem.tag for elem in tree.iter()]
    tatt=[elem.attrib for elem in tree.iter()]
```

tree.iter()會將遇到所有的元素組合都存成沒有層次的序列，一次破壞所有的層次，而將層次的從屬關係，變更序列前後的相對關係，同樣也可以利用前述序列的做法，將資料存成一龐大完整的表格。

不同的是vdid在本範例中是在屬性的範疇，而不是一個獨立的標籤，各個標籤中具有獨立特性的只有'Info'一項，2個'Info'中是一個完整的監測器數據內容，可以看做是一個封包，只是每個封包的大小不大相同。

因此首先必須先知道到底有幾個封包在檔案裏，然後逐一找到每個Info在序列中的位置，此2任務可以利用list.count與list.index指令。比較複雜一點的是，list.index只會找出序列裏第一個出現項目的位置，而對重複出現的項目，則必須先將之前找到的部分「摘除」，接著再找，找到後再加上前面摘掉的長度，如此疊代，即可找出所有項目的所在位置，如下所示：

```python
ist=[ttag.index('Info')]
for i in xrange(ttag.count('Info')-1):
    ist.append(ttag[ist[i]+1:].index('Info')+ist[i]+1)
ist.append(len(ttag))
```

範例程式中ist即是每個Info出現的位置，而所謂的「摘除」就是在進行ttag.index動作時，不計ttag序列在ist之前的內容，只計算ttag在ist+1以後的序列。

最後為了後續計算2個Info之間個數，需要ttag最後項的位置，即為ttag的長度，需要另外再附加上去。接下來就可以循序讀取封包裏各層屬性的內容了，類似前述循序讀取的程式：

```python
    v=[]
    for i in xrange(len(cols)):
        v.append([])
    for i in xrange(ttag.count('Info')):
        d2=tatt[ist[i]]
        ttl=(ist[i+1]-ist[i]-1)/4
        for lane in xrange(ttl):
            for j in xrange(3):
                v[j].append(d2[cols[j]])
            v[3].append(ttl)
            d3=tatt[ist[i]+4*lane+1]
            for j in xrange(4,8):
                v[j].append(d3[cols[j]])
            for k in xrange(ist[i]+4*lane+2,ist[i]+4*lane+5):
                d4=tatt[k]
                for l in xrange(len(cols)-3,len(cols)):
                    if d4['carid']==size[l]:v[l].append(d4['volume'])
```
- 在本例中並沒有一個數據是「總線道數」，必須要由封包大小計算，每個線道會佔用4個項目，因此只要將前後2個Info的出現位置相減，前後都不算，再減一，再除以4即可得到「總線道數」(ttl)
- 依序讀出封包內第2~4層的屬性，成為字典d2~d4，再依照cols的順序將其內容存在v序列中，如此 重複每一個線道，就可以順利排列成完整的大資料表，作法與前例相同。
- 值得注意的是桃園市將資料筆數寫得很工整，每一個線道一定都會有大、中、小車的標籤屬性，這才能使程式較容易撰寫，減少很多判斷式。然而也因此多了很多沒有必要的'0'值，必須將其刪除以減省儲存空間，這是與前述範例差異之處。

```python
    d={}
    for i in xrange(len(cols)):
        d.update({cols[i]:v[i]})
    df4=DataFrame(d)
    drp=[]
    for i in xrange(len(df4)):
        ss=int(df4.loc[i,'Svolume'])+int(df4.loc[i,'Mvolume'])+int(df4.loc[i,'Lvolume'])
        if ss==0 and int(df4.loc[i,'speed'])==0:
            drp.append(i)
    df4.drop(df4.index[drp], inplace=True)
    df4[cols].set_index('vdid').to_csv('latest')
```
- 範例程式中設定如果三種車型的總流量(ss)是0，而且速度也是0，表示並沒有量測值，就直接就地將其刪除，使用drop指令來刪除資料表的橫向列。為使刪除速度可以增進，此處將0值序號先記住，最後一併刪除，再儲存檔案。

## htm檔案之讀取
### 交通局網頁內容讀取案例
如前所示，BeautifulSoup是絕佳的html解碼器，以下範例即為台北市交通局網頁之內容，其後為讀取與整理程式。

```html
kuang@master /home/backup/data/ETC/TaipeiVD/htm
$ more sht3_10_NI001.htm
<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">
…
<tr height=30 style='mso-height-source:userset;height:23.1pt'>
  <td height=30 class=xl117 style='height:23.1pt'>D</td>
  <td class=xl117>▒@</td>
  <td class=xl117>▒@</td>
  <td class=xl121 align=right x:num>0</td>
  <td class=xl121 align=right x:num>227</td>
  <td class=xl121 align=right x:num>42</td>
  <td class=xl121 align=right x:num>362</td>
  <td class=xl158 align=right x:num="0.39999999999999991">40%</td>
  <td class=xl121>▒@</td>
  <td class=xl121>▒@</td>
  <td class=xl121 align=right x:num>3</td>
  <td class=xl121 align=right x:num>251</td>
  <td class=xl121 align=right x:num>6</td>
  <td class=xl121 align=right x:num>257</td>
  <td class=xl158 align=right x:num="0.29">29%</td>
  <td class=xl121>▒@</td>
  <td class=xl121 align=right x:num>2</td>
  <td class=xl121 align=right x:num>209</td>
  <td class=xl121 align=right x:num>8</td>
  <td class=xl121 align=right x:num>279</td>
  <td class=xl158 align=right x:num="0.31">31%</td>
  <td class=xl121>▒@</td>
  <td class=xl121 align=right x:num>5</td>
  <td class=xl158 align=right x:num="0.01">1%</td>
  <td class=xl121 align=right x:num>687</td>
  <td class=xl158 align=right x:num="0.91999999999999993">92%</td>
  <td class=xl121 align=right x:num>56</td>
  <td class=xl158 align=right x:num="0.07">7%</td>
  <td class=xl121 align=right x:num>898</td>
  <td class=xl122 align=right x:num="0.88217374213836486">0.88</td>
 </tr>
…
```

```python
kuang@master /home/backup/data/ETC/TaipeiVD/htm
$ cat rd_sht3.py
from bs4 import BeautifulSoup
from pandas import *
from pypinyin import pinyin, lazy_pinyin
import itertools

def FindLastABC(end):
    for last in xrange(end,-1,-1):
        if a[last] in ABC:break
    return last

with open('sss.txt') as ftext:
    s=[line.split('\n')[0] for line in ftext]

ABC=['A','B','C','D','E','F']
year,time,dirn,name,road=[],[],[],[],[]
nam_v=['Lvolume','Lratio','Mvolume','Mratio','Svolume','Sratio','PCU','PHF']
v=[]
for iv in xrange(8):
    v.append([])
note=['zhanming','zhan','shixiangshu']
for fname in s:
    yr=int(fname.split('_')[1])+90+1911
    if fname.split('.')[1]=='htm':
#section 1
        fn=open(fname,'r')
        soup = BeautifulSoup(fn,'html.parser')
        td=soup.find_all('td')
        a=[str(td[i]).split('>')[1].split('<')[0] for i in xrange(len(td))]
…
```
- 由範例中吾人可以發現，td與/td之間的內容是表格中的數值或文字，可以用td=soup.find_all('td')將其讀成序列，再利用split做2次切割，切出所要的內容(str(td[i]).split('>')[1].split('<')[0])。讀出a序列之後，接著進行道路名稱(站名)的解析：

```python
#section 2
    b=[]
    for i in xrange(100):
        cha=a[i]
        if type(cha)==float:
            b.append(cha)
            continue
        if len(cha)<2:
            b.append(cha)
            continue
        ss=lazy_pinyin(cha.decode('utf8'))
#section 3
        sss=''
        for j in ss:
            if j.isalnum():sss=sss+j
        b.append(sss)
#section 4
    for nt in note:
        try:
            b_note=b.index(nt)
        except:
            road_s='not_found'
        else:
            road_s=b[b_note+1]
            break
```
- section 2是先將序列中的數字分開，文字的長度如果小於2是單一字元，也不會是中文，就先儲存。其餘則運用中文拼音模組，將其翻譯為漢語拼音之字串(ss=lazy_pinyin(cha.decode('utf8')))。
- 由於ss是漢語字的序列，不方便進行比較與搜尋，section 3將其黏成一長字串sss，儲存在b序列。
- Section 4的目標是找出站名，一般會出現在「站名」之後，也有的htm出現在「站」或「時相數」之後，因此依序將其找出來，存在road_s變數以便儲存。

```python
#section 5
    last=FindLastABC(len(a)-1)
    A_last=a[last]
    lastend=len(a)
    if fname =='sht3_7_SI061.htm':A_last='D'
    ampm='pm'
    for chr in xrange(12): #repeat for A~F twice
        if fname == 'sht3_9_SI029.htm' and a[last]=='B':
            lastend=last
            last=FindLastABC(last-1)
            if a[last] == A_last and ampm=='am':break
            if a[last] == A_last and ampm=='pm':ampm='am'
            continue
#section 6
        tab=[]
        for j in xrange(last,lastend):
            try:
                tt=float(a[j])
            except:
                if len(a[j])>1 and a[j][-1]=='%':
                    tab.append(float(a[j][:-1])/100)
            else:
                tab.append(tt)
#section 7
        year.append(yr)
        time.append(ampm)
        dirn.append(a[last])
        name.append((fname.split('_')[2]).split('.')[0])
        road.append(road_s)
        if len(tab) <8:
            for iv in xrange(8-len(tab)):
                tab.insert(0,0.0)
        len_tab=len(tab)
        for gt1 in xrange(1,10):
            tt=tab[len(tab)-gt1]
            if 0< tt and tt < 1:
                len_tab=len_tab-gt1+1
                break
        if tab[len_tab-1] > 1 or tab[len_tab-1]==0.: tab=[0.0 for x in tab]
        for iv in xrange(len_tab-8,len_tab):
            v[iv-(len_tab-8)].append(tab[iv])
#section 8
        lastend=last
        last=FindLastABC(last-1)
        if a[last] == A_last and ampm=='am':break
        if a[last] == A_last and ampm=='pm':
            ampm='am'
#section 9
d={'year':year,'time':time,'dirn':dirn,'name':name,'road':road}
for iv in xrange(8):
    d.update({nam_v[iv]:v[iv]})
df=DataFrame(d)
for i in xrange(len(df)):
    sm=0.
    for j in nam_v:
        try:
            tt=float(df.loc[i,j])
        except:
            continue
        else:
            if tt>0:sm=sm+tt
    if sm==0.: df=df.drop(i)
cols=['year','time','dirn','name','road']+nam_v
df[cols].set_index('year').to_csv('sht3_df.csv')
```
- FindLastABC是一個在序列a中尋找字母的小副程式，只要從指標end以相反方向在序列中找到A~F中任一英文字母，就會回報該指標位置。藉由這支副程式，可以標定各方向的交通量統計結果。
- section 6的重點在將前述標定結果內容，讀成一序列tab。由於該序列並非全是數字，有可能有空格中文字，也有可能有%符號，必須以試誤方式來判斷。
- section 7目的在將各項數據儲存在序列裏。其中tab比較麻煩一點，有時會有數據不足(限制某種車輛進入車道)、或者數據太多(匝道交通量會出現在PHF之後)，因此需要逐一處理，原則就是，合計交通量最後一個數字一定是PHF，而該數字會是小於1的小數。
- section 8預備讀取下一個方向數據之前處理，同時也設停止點(break)。section 9就將讀取結果整理成資料庫形式，然後輸出。

### Wiki讀取交流道座標
```python
fnames=['01F.csv','01H.csv','02F.csv','03A.csv','03F.csv','04F.csv','05F.csv',
     '06F.csv','08F.csv','10F.csv']
guodao,sheshi,lat,lon=[],[],[],[]
for fname in fnames:
    a=read_csv(fname)
    c=a.columns[0]
    for nam in list(a[c]):
        url='https://zh.wikipedia.org/wiki/'+nam
        sheshi.append(nam)
        guodao.append(fname.replace('.csv',''))
        try:
            response = urllib2.urlopen(url)
        except:
            lat.append(0)
            lon.append(0)
        else:
            html = response.read()
            soup = BeautifulSoup(html,'html.parser')
            sp=soup.find_all('span')
            for i in xrange(len(sp)):
                if 'class' not in sp[i].attrs:continue
                if sp[i].attrs['class']==['latitude']:
                    for j in sp[i]:
                        lat.append(int(j[:2])+int(j[3:5])/60.+int(j[6:8])/3600.)
                if sp[i].attrs['class']==['longitude']:
                    for j in sp[i]:
                        lon.append(int(j[:3])+int(j[4:6])/60.+int(j[7:9])/3600.)
df_exitXY=DataFrame({'lat':lat,'lon':lon,'nam':nam,'guodao':guodao})
df_exitXY.set_index('guodao').to_csv('df_exitXY.csv')
```

### .json檔案之讀取與解析
桃園市2018年12月以後的VD檔案格式變成json格式，因此讀取解析程式也需要調整。
Json基本上就是dict，因此適用dict的操作方式：

```java
kuang@master /home/backup/data/ETC/TaoyuanVD
$ more latest
{
  "data" : [ {
    "vdid" : "68000V100110",
    "status" : "0",
    "datacollecttime" : "2019/01/25 13:40:00",
    "lane" : [ {
      "vsrdir" : "0",
      "vsrid" : "0",
      "speed" : "48",
      "laneoccupy" : "4.0",
      "cars" : [ {
        "carid" : "L",
        "volume" : "4"
      }, {
        "carid" : "S",
        "volume" : "28"
      }, {
        "carid" : "M",
        "volume" : "0"
      } ]
    }, {
      "vsrdir" : "0",
      "vsrid" : "1",
      "speed" : "45",
      "laneoccupy" : "4.0",
      "cars" : [ {
        "carid" : "L",
        "volume" : "1"
      }, {
        "carid" : "S",
        "volume" : "27"
      }, {
        "carid" : "M",
        "volume" : "0"
      } ]
    } ]
  }, {
    "vdid" : "68000V100360",
    "status" : "0",
    "datacollecttime" : "2019/01/25 13:40:00",
```
圖示latest檔案資料只有一項，key是data，對應到所有的內容，而latest[data]進去之後，則是個dict所形成的序列。